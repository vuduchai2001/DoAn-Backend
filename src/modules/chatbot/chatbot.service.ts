import { encodingForModel } from '@langchain/core/utils/tiktoken'
import { ChatOpenAI } from '@langchain/openai'
import { Injectable } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'
import { generatePrompt, modelName, modelNameTokens, temperature } from './chatbot.config'
import { ChatbotStreamConfig } from './dto/chatbot.dto'
import { HumanMessage } from '@langchain/core/messages'
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
  HumanMessagePromptTemplate,
} from '@langchain/core/prompts'
import { CatbotMongoChatMessageHistory } from './chat-message.service'
import { InjectModel } from '@nestjs/mongoose'
import { History } from '../history/schema/history.schema'
import { Model } from 'mongoose'
import { RunnableConfig, RunnableSequence, RunnableWithMessageHistory } from '@langchain/core/runnables'
import { StringOutputParser } from '@langchain/core/output_parsers'

@Injectable()
export class ChatbotService {
  private openAILlm: ChatOpenAI
  constructor(
    private configService: ConfigService,
    @InjectModel(History.name) private historyModel: Model<History>,
  ) {
    this.openAILlm = new ChatOpenAI({
      openAIApiKey: this.configService.get<string>('OPEN_AI_KEY'),
      model: modelName,
      temperature: temperature,
      streaming: false,
    })
  }

  async handleMessage(question: string) {
    const input = generatePrompt(question)

    const result = await this.openAILlm.invoke([new HumanMessage(input)])

    return result.content
  }

  async initializeChatbotStreamConfig(socketPayload: any) {
    return {
      promptTokens: 0,
      completionTokens: 0,
      totalTokens: 0,
      question: socketPayload.message,
      revisedQuestion: socketPayload.message,
      chatbotId: socketPayload.chatbotId,
      sessionId: socketPayload.sessionId,
      // roomName: socketPayload.roomName,
    }
  }

  async countTokenUsage(message: string): Promise<number> {
    const enc = await encodingForModel(modelNameTokens)
    const nunOfTokens = enc.encode(message).length
    return nunOfTokens
  }

  private createChatOpenAI(streamConfig: ChatbotStreamConfig) {
    return new ChatOpenAI({
      temperature: temperature,
      model: modelName,
      streaming: true,
      openAIApiKey: this.configService.get<string>('OPEN_AI_KEY'),
      callbacks: [
        {
          handleChatModelStart: async (_llm, messages) => {
            for (const messageArray of messages) {
              for (const message of messageArray) {
                const content = message.content as string
                const tokenCount = await this.countTokenUsage(content)
                streamConfig.promptTokens += tokenCount
                streamConfig.totalTokens += tokenCount
              }
            }
          },
          handleLLMEnd: async (outputs) => {
            let outputText = outputs.generations[0][0].text
            const tokenCount = await this.countTokenUsage(JSON.stringify(outputText))
            outputText = this.formatLLMOutput(outputText)
            streamConfig.completionTokens += tokenCount
            streamConfig.totalTokens += tokenCount
          },
        },
      ],
    })
  }

  private formatLLMOutput(text: string): string {
    /*
   *This function formats the output text from an LLM, including:
   *
   *- Removing "Answer: " or "A: "  at the beginning (if present)
   *- Capitalizing the first letter of the sentence
   *- Trimming extra spaces and newlines

   *Args:
   *    text (str): The text generated by the LLM.
   *
   *Returns:
   *    str: The formatted text.
  */

    // Remove extra spaces and newlines
    let processedText = text.trim()

    // Remove "Answer: " at the beginning (if present)
    const prefixes = ['Answer: ', 'A: ']
    for (const prefix of prefixes) {
      if (processedText.startsWith(prefix)) {
        processedText = processedText.slice(prefix.length)
        break // Remove only the first matching prefix
      }
    }

    // Capitalize the first letter of the sentence
    processedText = processedText[0].toUpperCase() + processedText.slice(1)
    return processedText
  }

  async createLLMChain(streamConfig: ChatbotStreamConfig) {
    const prompt = ChatPromptTemplate.fromMessages([
      SystemMessagePromptTemplate.fromTemplate(generatePrompt(streamConfig.revisedQuestion)),
      new MessagesPlaceholder('history'),
      HumanMessagePromptTemplate.fromTemplate('{question}'),
    ])
    const llm = this.createChatOpenAI(streamConfig)

    const messageHistory = new CatbotMongoChatMessageHistory({
      collection: this.historyModel.collection,
      sessionId: streamConfig.sessionId,
    })

    const runnable = RunnableSequence.from([prompt, llm, new StringOutputParser()])

    const config: RunnableConfig = { configurable: { sessionId: streamConfig.sessionId } }
    const chainWithHistory = new RunnableWithMessageHistory({
      runnable: runnable,
      getMessageHistory: () => messageHistory,
      inputMessagesKey: 'question',
      historyMessagesKey: 'history',
      config,
    })

    return chainWithHistory
  }
}
